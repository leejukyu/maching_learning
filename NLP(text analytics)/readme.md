## 1. 용어 정리
#### 1) 공부할 부분
- 텍스트 분류(Text Classification): 문서가 특정 카테고리에 속하는 것을 예측, 지도학습 ex) 스팸 검출, 신문 기사 분류
- 감정 분석(Sentiment Analysis): 텍스트에서 나타나는 주관적 요소를 분석, 지도, 비지도 학습
- 텍스트 요약(Summarization): 중요한 주제나 중심 사상을 추출하는 기법 ex) 토픽 모델링
- 텍스트 군집화(Clustering)와 유사도 측정: 비슷한 유형의 문서에 대해 군집화 수행, 비지도 학습
#### 2) 수행 프로세스
- (1) 텍스트 사전 준비작업(전처리): 클렌징, 대/소문자 변경, 특수문자 삭제 등의 클렌징 작업
- (2) 피처 벡터화/추출: 텍스트를 다수의 피처로 추출하고 빈도수와 같은 숫자 값을 부여하여 벡터값으로 변환하는 단계
  - BOW - Count, TF-IDF
  - Word2Vec
- (3) MS 모델 수립 및 학습/예측/평가
#### 3) 대표적인 NLP와 텍스트 분석 패키지
- NLTK(Natural Language Toolkit for Python): 파이썬의 대표적인 NLP패키지, 거의 모든 영역 커버, *수행 속도가 느려 실제 대량의 데이터에선 활용하기 힘듬*
- Gensim: *토픽* 모델링 분야에서 두각
- SpaCy: 뛰어난 수행 성능으로 최근 가장 주목
## 2. [텍스트 사전 준비작업 - 텍스트 정규화](text preprocessing.ipynb)
#### 1) 클렌징: 불필요한 문자, 기호 등을 사전에 제거
#### 2) 텍스트 토큰화
- 문장 토큰화: 마침표, 개행문자 등 문장의 마지막을 뜻하는 기호에 따라 분리하는 것이 일반적
- 단어 토큰화: 기본적으로 공백, 콤마, 마침표, 개행문자 등으로 단어를 분리하지만, 정규 표현식을 이용해 다양한 유형으로 수행, 단어의 순서가 중요하지 않으면 문장 토큰화를 하지 않고 바로 단어 토큰화를 해도 충분
- n_gram: 단어별로 토큰화 하면 문맥적인 의미가 무시됨 -> 연속된 n개의 단어를 하나의 토큰화 단위로 분리
#### 3) 스톱 워드 제거: 분석에 큰 의미가 없는 단어 제거
#### 4) Stemming과 Lemmatization: 문법적 또는 의미적으로 변화하는 단어의 원형을 찾음
- Lemmatization: Stemming보다 정교하며 의미론적인 기반에서 단어의 원형을 찾음, 더 오래 걸림, WordNetLemmatizer
- Stemmer: Porter, Lancaster, Snowball Stemmer
